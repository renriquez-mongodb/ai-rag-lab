"use strict";(self.webpackChunkai_rag_lab=self.webpackChunkai_rag_lab||[]).push([[5951],{3943:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/ai-rag-lab/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Retrieval Augmented Generation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udcd8 What is RAG?","href":"/ai-rag-lab/docs/rag/what-is-rag","docId":"rag/what-is-rag","unlisted":false},{"type":"link","label":"\ud83d\udcd8 When to use RAG?","href":"/ai-rag-lab/docs/rag/rag-usecases","docId":"rag/rag-usecases","unlisted":false},{"type":"link","label":"\ud83d\udcd8 Components of a RAG system","href":"/ai-rag-lab/docs/rag/components-of-rag","docId":"rag/components-of-rag","unlisted":false}],"href":"/ai-rag-lab/docs/category/retrieval-augmented-generation"},{"type":"category","label":"MongoDB Atlas","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udc50 Create your account","href":"/ai-rag-lab/docs/mongodb-atlas/create-account","docId":"mongodb-atlas/create-account","unlisted":false},{"type":"link","label":"\ud83d\udc50 Deploy a database cluster","href":"/ai-rag-lab/docs/mongodb-atlas/create-cluster","docId":"mongodb-atlas/create-cluster","unlisted":false},{"type":"link","label":"\ud83d\udc50 Get your connection string","href":"/ai-rag-lab/docs/mongodb-atlas/get-connection-string","docId":"mongodb-atlas/get-connection-string","unlisted":false}],"href":"/ai-rag-lab/docs/category/mongodb-atlas"},{"type":"category","label":"Fireworks AI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udc50 Create an account","href":"/ai-rag-lab/docs/fireworks-ai/create-account","docId":"fireworks-ai/create-account","unlisted":false},{"type":"link","label":"\ud83d\udc50 Create an API key","href":"/ai-rag-lab/docs/fireworks-ai/create-api-key","docId":"fireworks-ai/create-api-key","unlisted":false}],"href":"/ai-rag-lab/docs/category/fireworks-ai"},{"type":"category","label":"Dev Environment","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udc50 Setup dev environment","href":"/ai-rag-lab/docs/dev-env/dev-setup","docId":"dev-env/dev-setup","unlisted":false},{"type":"link","label":"\ud83d\udc50 Setup prerequisites","href":"/ai-rag-lab/docs/dev-env/setup-pre-reqs","docId":"dev-env/setup-pre-reqs","unlisted":false}],"href":"/ai-rag-lab/docs/category/dev-environment"},{"type":"category","label":"Prepare the Data","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udcd8 Running Jupyter Notebooks in Google Colab","href":"/ai-rag-lab/docs/prepare-the-data/jupyter-notebooks","docId":"prepare-the-data/jupyter-notebooks","unlisted":false},{"type":"link","label":"\ud83d\udc50 Load the dataset","href":"/ai-rag-lab/docs/prepare-the-data/load-data","docId":"prepare-the-data/load-data","unlisted":false},{"type":"link","label":"\ud83d\udc50 Chunk up the data","href":"/ai-rag-lab/docs/prepare-the-data/chunk-data","docId":"prepare-the-data/chunk-data","unlisted":false},{"type":"link","label":"\ud83d\udc50 Generate embeddings","href":"/ai-rag-lab/docs/prepare-the-data/embed-data","docId":"prepare-the-data/embed-data","unlisted":false},{"type":"link","label":"\ud83d\udc50 Ingest data into MongoDB","href":"/ai-rag-lab/docs/prepare-the-data/ingest-data","docId":"prepare-the-data/ingest-data","unlisted":false}],"href":"/ai-rag-lab/docs/category/prepare-the-data"},{"type":"category","label":"Perform Semantic Search on Your Data","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udcd8 Semantic search in MongoDB","href":"/ai-rag-lab/docs/perform-semantic-search/concepts","docId":"perform-semantic-search/concepts","unlisted":false},{"type":"link","label":"\ud83d\udc50 Create a vector search index","href":"/ai-rag-lab/docs/perform-semantic-search/create-vector-index","docId":"perform-semantic-search/create-vector-index","unlisted":false},{"type":"link","label":"\ud83d\udc50 Perform semantic search","href":"/ai-rag-lab/docs/perform-semantic-search/vector-search","docId":"perform-semantic-search/vector-search","unlisted":false},{"type":"link","label":"\ud83e\uddb9 Combine pre-filtering with vector search","href":"/ai-rag-lab/docs/perform-semantic-search/pre-filtering","docId":"perform-semantic-search/pre-filtering","unlisted":false}],"href":"/ai-rag-lab/docs/category/perform-semantic-search-on-your-data"},{"type":"category","label":"Build the RAG Application","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udc50 Build the RAG application","href":"/ai-rag-lab/docs/build-rag-app/build-rag-app","docId":"build-rag-app/build-rag-app","unlisted":false},{"type":"link","label":"\ud83e\uddb9 Re-rank retrieved results","href":"/ai-rag-lab/docs/build-rag-app/add-reranking","docId":"build-rag-app/add-reranking","unlisted":false},{"type":"link","label":"\ud83e\uddb9 Stream responses from the RAG application","href":"/ai-rag-lab/docs/build-rag-app/stream-responses","docId":"build-rag-app/stream-responses","unlisted":false}],"href":"/ai-rag-lab/docs/category/build-the-rag-application"},{"type":"category","label":"Add memory to the RAG application","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"\ud83d\udc50 Add memory to the RAG application","href":"/ai-rag-lab/docs/add-memory/add-memory","docId":"add-memory/add-memory","unlisted":false}],"href":"/ai-rag-lab/docs/category/add-memory-to-the-rag-application"},{"type":"link","label":"\ud83c\udfaf Summary","href":"/ai-rag-lab/docs/summary","docId":"summary","unlisted":false}]},"docs":{"add-memory/add-memory":{"id":"add-memory/add-memory","title":"\ud83d\udc50 Add memory to the RAG application","description":"In many Q&A applications we want to allow the user to have a back-and-forth conversation with the LLM, meaning the application needs some sort of \\"memory\\" of past questions and answers, and some logic for incorporating those into its current thinking. In this section, you will retrieve chat message history from MongoDB and incorporate it in your RAG application.","sidebar":"tutorialSidebar"},"build-rag-app/add-reranking":{"id":"build-rag-app/add-reranking","title":"\ud83e\uddb9 Re-rank retrieved results","description":"Re-rankers are specialized models that are trained to calculate the relevance between query-document pairs. Without re-ranking the order of retrieved results is governed by the embedding model, which isn\'t optimized for relevance and can lead to poor LLM recall in RAG applications.","sidebar":"tutorialSidebar"},"build-rag-app/build-rag-app":{"id":"build-rag-app/build-rag-app","title":"\ud83d\udc50 Build the RAG application","description":"Let\'s create a simple RAG application that takes in a user query, retrieves contextually relevant documents from MongoDB Atlas, and passes the query and retrieved context to the Llama 3 8B Instruct model to generate an answer to the user question.","sidebar":"tutorialSidebar"},"build-rag-app/stream-responses":{"id":"build-rag-app/stream-responses","title":"\ud83e\uddb9 Stream responses from the RAG application","description":"By default, generation results are returned once the generation is completed. Another option is to stream the results as they come, which is useful for chat use cases where the user can incrementally see results as each token is generated.","sidebar":"tutorialSidebar"},"dev-env/dev-setup":{"id":"dev-env/dev-setup","title":"\ud83d\udc50 Setup dev environment","description":"You will be working in a Jupyter Notebook throughout this lab. The easiest and recommended way to run the lab notebook is using Google Colab.","sidebar":"tutorialSidebar"},"dev-env/setup-pre-reqs":{"id":"dev-env/setup-pre-reqs","title":"\ud83d\udc50 Setup prerequisites","description":"Replace any placeholders and run the cells under the Step 1 Setup prerequisites sections in the notebook.","sidebar":"tutorialSidebar"},"fireworks-ai/create-account":{"id":"fireworks-ai/create-account","title":"\ud83d\udc50 Create an account","description":"In this lab, we will use the Llama 3 8B Instruct model hosted by Fireworks AI.","sidebar":"tutorialSidebar"},"fireworks-ai/create-api-key":{"id":"fireworks-ai/create-api-key","title":"\ud83d\udc50 Create an API key","description":"Once logged in, navigate to API keys under User Settings.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"|Lab goals|Learn how to build a RAG application from scratch|","sidebar":"tutorialSidebar"},"mongodb-atlas/create-account":{"id":"mongodb-atlas/create-account","title":"\ud83d\udc50 Create your account","description":"In this lab, you will learn how to use MongoDB Atlas as a knowledge base as well as a memory provider for RAG applications.","sidebar":"tutorialSidebar"},"mongodb-atlas/create-cluster":{"id":"mongodb-atlas/create-cluster","title":"\ud83d\udc50 Deploy a database cluster","description":"Now that you have a MongoDB Atlas account, you can create your first cluster for free.","sidebar":"tutorialSidebar"},"mongodb-atlas/get-connection-string":{"id":"mongodb-atlas/get-connection-string","title":"\ud83d\udc50 Get your connection string","description":"In order to ingest data into your cluster later in the lab, you will need to get the connection string for your cluster.","sidebar":"tutorialSidebar"},"perform-semantic-search/concepts":{"id":"perform-semantic-search/concepts","title":"\ud83d\udcd8 Semantic search in MongoDB","description":"In MongoDB, you can semantically search through your data using MongoDB Atlas Vector Search.","sidebar":"tutorialSidebar"},"perform-semantic-search/create-vector-index":{"id":"perform-semantic-search/create-vector-index","title":"\ud83d\udc50 Create a vector search index","description":"To retrieve documents from MongoDB using vector search, you must configure a vector search index on the collection into which you ingested your data. The recommended way to do this is via the MongoDB drivers.","sidebar":"tutorialSidebar"},"perform-semantic-search/pre-filtering":{"id":"perform-semantic-search/pre-filtering","title":"\ud83e\uddb9 Combine pre-filtering with vector search","description":"Pre-filtering is a technique to optimize vector search by only considering documents that match certain criteria during vector search.","sidebar":"tutorialSidebar"},"perform-semantic-search/vector-search":{"id":"perform-semantic-search/vector-search","title":"\ud83d\udc50 Perform semantic search","description":"Now let\'s run some vector search queries against our data present in MongoDB.","sidebar":"tutorialSidebar"},"prepare-the-data/chunk-data":{"id":"prepare-the-data/chunk-data","title":"\ud83d\udc50 Chunk up the data","description":"Since we are working with large documents, we first need to break them up into smaller chunks before embedding and storing them in MongoDB.","sidebar":"tutorialSidebar"},"prepare-the-data/embed-data":{"id":"prepare-the-data/embed-data","title":"\ud83d\udc50 Generate embeddings","description":"To perform vector search on our data, we need to embed it (i.e. generate embedding vectors) before ingesting it into MongoDB.","sidebar":"tutorialSidebar"},"prepare-the-data/ingest-data":{"id":"prepare-the-data/ingest-data","title":"\ud83d\udc50 Ingest data into MongoDB","description":"The final step to build a MongoDB vector store for our RAG application is to ingest the embedded article chunks into MongoDB.","sidebar":"tutorialSidebar"},"prepare-the-data/jupyter-notebooks":{"id":"prepare-the-data/jupyter-notebooks","title":"\ud83d\udcd8 Running Jupyter Notebooks in Google Colab","description":"Jupyter Notebooks is an interactive Python environment. Cells in a Jupyter notebook are a modular unit of code or text that you can execute and view outputs for.","sidebar":"tutorialSidebar"},"prepare-the-data/load-data":{"id":"prepare-the-data/load-data","title":"\ud83d\udc50 Load the dataset","description":"First, let\'s download the dataset for our lab. We\'ll use a subset of articles from the MongoDB Developer Center as the source data for our RAG application.","sidebar":"tutorialSidebar"},"rag/components-of-rag":{"id":"rag/components-of-rag","title":"\ud83d\udcd8 Components of a RAG system","description":"RAG systems have two main components: Retrieval and Generation.","sidebar":"tutorialSidebar"},"rag/rag-usecases":{"id":"rag/rag-usecases","title":"\ud83d\udcd8 When to use RAG?","description":"RAG is best suited for the following:","sidebar":"tutorialSidebar"},"rag/what-is-rag":{"id":"rag/what-is-rag","title":"\ud83d\udcd8 What is RAG?","description":"RAG, short for Retrieval Augmented Generation, is a technique to enhance the quality of responses generated by a large language model (LLM), by augmenting its pre-trained knowledge with information retrieved from external sources. This results is more accurate responses from the LLM by grounding them in real, contextually relevant data.","sidebar":"tutorialSidebar"},"summary":{"id":"summary","title":"\ud83c\udfaf Summary","description":"Congratulations! Following this lab, you have successfully:","sidebar":"tutorialSidebar"}}}}')}}]);